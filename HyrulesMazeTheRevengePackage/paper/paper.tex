\def\year{2015}
%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage[utf8]{inputenc}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Planejamento em um Mundo Baseado em Portal (Valve))
/Author (Guilherme de M. M. Taschetto)}
\setcounter{secnumdepth}{0}
\begin{document}

\title{Aprendizagem por Reforço\\Active Temporal Difference Learning}
\author{Guilherme de M. M. Taschetto, João Pedro Chagas\\
Acadêmicos da Faculdade de Informática da\\
Pontifícia Universidade Católica do Rio Grande do Sul\\
Av. Ipiranga, 6681\\
Porto Alegre, Rio Grande do Sul 90619-900}

\maketitle
\begin{abstract}
Este relatório visa apresentar o paso a paso utilizado pela equipe para o desenvolvimento o algoritmo de aprendizagem por
reforço, desde a concepção do mesmo até a implementalção.
\end{abstract}

\section{Introdução}

Tendo em vista a necessidade de desenvolver um algoritmo de aprendizagem por reforço, o grupo decidiu por fazer a implementação
do TD Learning, pois o mesmo é mais preciso apesar de demorar mais tempo para convergir para o resultado final, já que ele se baseia
em ajustar as previsões para que seja mais perto possível do resultado esperado.

\section{Descrição do Algoritmo}

O TD Learning tem como funcionamento, utilizando uma política inicial “randômica”, chegar até o objetivo, ou seja, onde a recompensa é
maior. Ao chegar neste objetivo o algoritmo deve remontar o caminho que ele fez, neste processo o mesmo calcula as utilidades de cada estado
que ele passou comparando com o estado anterior. Após efetuar esse primeiro caminho o mesmo se utiliza dos estados e utilidades já computados
e verifica outras opções vendo a probabilidade de ir para outro estado, sendo assim atualizando a política que ele utilizou no início para uma
regra mais direcionada para o objetivo que o mesmo deseja obter. Após atualizar a política, o algoritmo executa novamente o caminho feito na
nova regra, calculando as novas utilidades dos estados. Isto será efetuado até que os valores convirjam para o mesmo e ele comece a seguir
sempre a mesma política.

No caso do algoritmo implementado, em um primeiro momento ele executa uma política randômica até chegar no objetivo. Chegando lá o método
updateUtilities é chamado para que seja calculado os valores das utilidades dos estados que foram percorridos. Ao finalizar a atualização das
políticas é feita o recálculo da política através do método recalculatePolicy. O algoritmo vai ficar neste processo de atualização de utilidades
e recálculo de politicas até que os valores de utilidades dos estados convirjam. Para realizar esse processo foram utilizados alguns métodos
auxiliares. Dentre eles esta o avaibleActions que retorna um conjunto de ações possíveis a partir de um determinado estado. O método Pi retorna
a melhor ação possível para um determinado estado selecionado a maior recompensa obtida de todos os estados que é obtido através do método E. E
por fim existe o método sucessor que faz com que seja possível andar pelos estados que a política está selecionando como melhor estado até
aquele momento.

\section{Avaliação do Algoritmo}

\section{Conclusão}

Uma das maiores dificuldades encontradas para o desenvolvimento do algoritmo foi a verificação de se o mesmo estava correto ou não, já que o mesmo
demora para convergir.

\end{document}