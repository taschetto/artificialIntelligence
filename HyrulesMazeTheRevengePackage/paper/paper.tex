\def\year{2014}
%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage[utf8]{inputenc}
\usepackage{fancyvrb}
\usepackage{listings}
\lstset {
    mathescape,
    frame=none
}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (prendizagem por Reforço Active Temporal Difference Learning)
/Author (Guilherme de M. M. Taschetto, João Pedro Chagas)}
\setcounter{secnumdepth}{0}
\begin{document}

\title{Aprendizagem por Reforço\\Active Temporal Difference Learning}
\author{Guilherme de M. M. Taschetto, João Pedro Chagas\\
Acadêmicos da Faculdade de Informática da\\
Pontifícia Universidade Católica do Rio Grande do Sul\\
Av. Ipiranga, 6681\\
Porto Alegre, Rio Grande do Sul 90619-900}

\maketitle
\begin{abstract}
Este artigo tem por objetivo apresentar uma aplicação de um algoritmo de Aprenziado por Reforço
no contexto de um labirinto onde o personagem Link deve encontrar um baú através de um caminho
de forma a maximizar a recompensa final obtida. Além dos argumentos para a escolha do algoritmo,
são apresentados detalhes da implementação e apresentação e análise dos resultados obtidos.
\end{abstract}

\section{Introdução}

Nosso agente, Link, tem por objetivo caminhar por um labirinto e chegar ao final (marcado pela
presença de um baú) maximizando a recompensa obtida. A recompensa pode ser representada da
seguinte forma:

\begin{itemize}
\item +50 ao encontrar o baú;
\item +5 ao encontrar uma \textit{rupee}\footnote{Uma espécie de pedra preciosa.};
\item -0.1 ao caminhar ou dar de cada com uma parede.
\end{itemize}

Além disso, ao tentar mover-se em uma direção X, Link pode...

\begin{itemize}
\item avançar na direção X, com 70\% de chances que isso ocorra;
\item avançar em uma direção perpendicular à X, com 30\% de chances que isso ocorra - sendo 15\%
de chance para cada lado.
\end{itemize}

É neste contexto que desejamos aplicar um algoritmo de Aprendizado por Reforço. Através de
sucessivas experiências - ou seja, andar e conhecer o labirinto e suas caracterísricas - Link
deverá ser capaz de realizar aprimoraramentos o resultado obtido por seu caminhamento - ou seja,
aprender com a sua própria experiência.

\section{Escolha do Algoritmo}

Dentre as opções possíveis de algoritmos de Aprendizado por Reforço optamos pelo
\textit{Active TD Learning}\footnote{\textit{Active Temporal Difference Learning}, ou Aprendizado
Ativo por Diferença Temporal.}. Os principais fatores que levaram à escolha forama maior
simplicidade de compreensão e implementação, além de apresentar uma maior precisão ao ser
comparado com algoritmos similares, já que se baseia em ajustar as utilidades dos estados para
que o mais próximo possível do resultado esperado (após a convergência).

\section{Descrição do Algoritmo}

O \textit{ATDL} pode ser dividido em duas grandes partes:

\begin{description}
\item[Atualização das Utilidades] Etapa na qual o agente transita no espaço de estados,
seguindo uma política $\pi$, e é feito o recálculo das utilidades dos estados.
\item[Atualização da Política] Etapa na qual é criada uma nova política baseada nos
novos valores de utilidades.
\end{description}

\subsection{Atualização das Utilidades}

Esta etapa faz uso de alguns artefatos. São eles:

\begin{description}
\item[$\pi$] Uma política inicial qualquer.
\item[$U(s)$] Utilidade para o estado $s$ (inicialmente $0$).
\item[$N(s)$] Número de visitas ao estado $s$.
\item[$\alpha$] Taxa de aprendizado (normalmente $1/(N(s)+1)$).
\end{description}

O algoritmo em si é descrito pelo seguinte pseudo-código:

\begin{lstlisting}
$if\ s\prime\ is\ new\ then\ U[s\prime]\ \leftarrow\ r\prime$
$if\ s\ is\ not\ null\ then$
$\ \ \ \ increment\ N_s[s]$
$\ \ \ \ U[s]\ \leftarrow\ U[s]\ +\ \alpha (N_s[s])(r\ +\ \gamma U[s]\ -\ U[s])$
\end{lstlisting}

Em linhas gerais, o algoritmo transita entre os estados utilizando a política $\pi$. Em cada
transição de estados, i. e. $s \rightarrow s\prime$, o algoritmo é executado. Descritivamente,
o algoritmo verifica se o estado atingido nunca havia sido atingido antes - em caso positivo,
a utilidade do estado é a própria recompensa imediata do estado. E, caso o estado anterior seja
válido (ou seja, não nulo), é feito o incremento no contador de visitas daquele estado e
atualizada a utilidade do mesmo seguindo a fórmula do algoritmo.

Entretanto, para tornar o algoritmo mais flexível e fazer com que ele busque por novos caminhos
(que podem ser benéficos), é adicionada uma função de exploração:

\[U[s]\ \leftarrow\ U[s]\ +\ \alpha (N_s[s])(r\ +\ \gamma U[s]\ -\ f(U[s],\ N[s]))\]

\subsection{Atualização da Política}

Para tornar a atualização das utilidades dos estados algo útil deve ser gerada uma nova política
com base nestes valores. Uma política compreende um mapeamento $\pi(s) \rightarrow Action$, ou
seja, a política define uma ação para cada estado.

O \textit{ATDL} define que, de tempos em tempos, a política deve ser recalculada. Para isto, é
necessário realizar o processamento de um MDP\footnote{\textit{Markov Decision Process}, ou
Processo de Decisão de Markov.} A nova política para um estado $s$ se dá por:

\[\pi(s) = arg\ max_a \sum\limits_{s\prime} P(s\prime|s,a) * U(s\prime)\]

Ou seja, a política do estado $s$ é a ação que maximiza a utilidade esperada. A função $P(s\prime|s,a)$
traduz, na fórmula, as probabilidades descritas no início deste relatório (70\% + 15\% + 15\%).

\section{Descrição da Implementação}

Em nossa implementação criamos uma classe \texttt{Tdlr} para servir como um \textit{façade} para
as funções do algoritmo. O algoritmo pode ser configurado pelos seguintes parâmetros:

\begin{description}
  \item[$float\ gamma\ =\ .9f;$] \hfill \\
  Fator utilizado para acelerar a convergência.
  \item[$int\ policySteps\ =\ 1;$] \hfill \\
  De quantos em quantos passos a política será recalculada.
  \item[$int\ explorationThreshold\ =\ 20;$] \hfill \\
  O limiar de exploração utilizado pela função de exploração.
  \item[$float\ maximumReward\ =\ 55;$] \hfill \\
  A recompensa máxima, também utilizada pela função de exploração.

\end{description}

\subsection{Estruturas de Dados}

\begin{description}
\item[$\pi$] \hfill \\ $HashMap$ encapsulado pela classe $DynamicPolicy$.
\item[$U(s)$] \hfill \\ $HashMap<State, Float> U$.
\item[$N(s)$] \hfill \\ $HashMap<State, Integer> N$.
\item[$\alpha$] \hfill \\ Método $Tdlr\#alpha(int n)$.
\end{description}

\subsection{Interface Pública}

\begin{description}
\item[$Tdlr\#updateUtilities(s\prime,\ s)$] \hfill\\
  Método que executa a parte I do algoritmo, atualizando as utilidades dos estados recebidos
  nos parâmetros.
\item[$Tdlr\#updatePolicy(tileMap, currentPolicy)$] \hfill\\
  Método que atualiza a política. O parâmetro $tileMap$ é utilizado para saber o tamanho do
  labirinto. O parâmetro $currentPolicy$ contém a política atual. Este método é chamado a cada
  passo dado por Link. Porém, internamente, o método utiliza um contador parametrizável para
  realizar o cálculo a cada $policySteps$ passos.
\end{description}

\section{Avaliação do Algoritmo}

\section{Conclusão}

Uma das maiores dificuldades encontradas para o desenvolvimento do algoritmo foi a verificação de se o mesmo estava correto ou não, já que o mesmo
demora para convergir.

\end{document}